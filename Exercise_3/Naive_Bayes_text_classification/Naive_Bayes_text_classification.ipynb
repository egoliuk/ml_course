{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/eugene/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from utils import load_dataset, split_dataset, split_dataset_data_frame\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus shape (1118, 2)\n",
      "train shape (895, 1)\n",
      "test shape (223, 1)\n"
     ]
    }
   ],
   "source": [
    "# Upload datasets\n",
    "\n",
    "train_messages, train_labels, test_messages, test_labels, corpus = load_dataset()\n",
    "print('corpus shape', corpus.shape)\n",
    "print('train shape', train_messages.shape)\n",
    "print('test shape', test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_normalize(text, remove_small_words = True, leave_only_letters = True, to_lower_case = True):\n",
    "    \"\"\"\n",
    "    Converting a sentence into list of words. Normalize text.\n",
    "    \n",
    "    Argument:\n",
    "    text -- a sentence that should be tokenized and normalized\n",
    "    to_lower_case -- reduced all words to lowercase. Default value is True\n",
    "    leave_only_letters -- remove all irrelevant characters (any non-letter characters). Default value is True\n",
    "    remove_small_words -- remove all small words (less than 3 characters). Default value is True\n",
    "    \n",
    "    Returns:\n",
    "    words -- list of words\n",
    "\n",
    "    \"\"\"\n",
    "    if to_lower_case:\n",
    "        text=text.lower()\n",
    "    pattern = r'[A-Z,a-z]' if leave_only_letters else r'\\S' \n",
    "    pattern += r'{3,}' if remove_small_words else r'{1,}' \n",
    "    words=re.findall(pattern,text)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in row string:  481\n",
      "number of words in normalized string:  372\n"
     ]
    }
   ],
   "source": [
    "print('number of words in row string: ', len(train_messages[3, 0].split()))\n",
    "words = tokenize_and_normalize(train_messages[3, 0])\n",
    "print('number of words in normalized string: ', len(words))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words in string before remove stopwords: 372\n",
      "number of words in string after stopwords have been removed: 358\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(row_words):\n",
    "    \"\"\"\n",
    "    Remove stopwords from list of words.\n",
    "    \n",
    "    Argument:\n",
    "    row_words -- a list of words that contains stopwords that should be removed\n",
    "    \n",
    "    Returns:\n",
    "    words -- list of words\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    clean_words = row_words.copy()\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords = tokenize_and_normalize(' '.join(stopwords))\n",
    "    stopwords = list(set(stopwords))\n",
    "    \n",
    "    clean_words = [x for x in clean_words if x not in stopwords]\n",
    "    \n",
    "    return clean_words\n",
    "\n",
    "print('number of words in string before remove stopwords:', len(words))\n",
    "words = remove_stopwords(words)\n",
    "print('number of words in string after stopwords have been removed:', len(words))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [labels, messages]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'messages' : [], 'labels' : []})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the NaiveBayes class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.corpus = pd.DataFrame({'messages' : [], 'labels' : []})\n",
    "        self.classes = np.empty(0)\n",
    "        self.tokens = []\n",
    "        self.frequency_table = pd.DataFrame({'col' : []})\n",
    "        self.likelihoods_of_tokens = []\n",
    "        self.likelihoods_of_classes = []\n",
    "        self.likelihoods_of_tokens_for_each_classes = []\n",
    "    \n",
    "    def fit(self, corpus):\n",
    "        self.corpus = corpus.copy(deep=True)\n",
    "        self.classes = self.corpus['labels'].unique()\n",
    "        train_corpus, validation_corpus = self.split_corpus(self.corpus)\n",
    "        \n",
    "        self.frequency_table, self.tokens = self.__to_frequency_table(self.corpus)\n",
    "        self.likelihoods_of_tokens = self.__calc_likelihoods_of_tokens()\n",
    "        self.likelihoods_of_classes = self.__calc_likelihoods_of_classes()\n",
    "        self.likelihoods_of_tokens_for_each_classes = self.__calc_likelihoods_of_tokens_for_each_classes()\n",
    "        \n",
    "        accuracy = 0.01\n",
    "        return accuracy\n",
    "    \n",
    "    def predictOne(self, text):\n",
    "#         print('!!!!!!!!!!! call predictOne with tokenized text', tokenize_and_normalize(text))\n",
    "        likelihoods_of_classes = self.__calc_likelihoods_of_classes_for_each_tokens(tokenize_and_normalize(text))\n",
    "#         print('!!!!!!!!!!! likelihoods_of_classes', likelihoods_of_classes)\n",
    "        predicted_class = likelihoods_of_classes.idxmax(axis=0)\n",
    "        return predicted_class.values[0]\n",
    "\n",
    "    \n",
    "    def predict(self, corpus):\n",
    "        x = np.array(corpus).reshape(len(corpus), 1)\n",
    "        y = np.repeat(-1, len(corpus)).reshape(len(corpus), 1)\n",
    "        data = np.append(x, y, axis=1)\n",
    "        \n",
    "        corpus_with_predicted_classes = pd.DataFrame(data=data,\n",
    "                                                     index=np.arange(len(corpus)),\n",
    "                                                     columns=[\"messages\", \"predicted classes\"])\n",
    "        \n",
    "#         print('!!!!!!!!!!! corpus_with_predicted_classes', corpus_with_predicted_classes)\n",
    "#         print('!!!!!!!!!!! enumerate(corpus', enumerate(corpus))\n",
    "\n",
    "        for idx, text in enumerate(corpus):\n",
    "#             print('!!!!!!!!!!! TEXT FOR PREDICT', corpus_with_predicted_classes.at[idx, \"messages\"])\n",
    "            clazz = self.predictOne(corpus_with_predicted_classes.at[idx, \"messages\"])\n",
    "            corpus_with_predicted_classes.at[idx, \"predicted classes\"] = clazz\n",
    "\n",
    "        return corpus_with_predicted_classes\n",
    "    \n",
    "    def __to_frequency_table(self, inputCorpus):\n",
    "        corpus = inputCorpus.copy(deep=True)\n",
    "        tokens = tokenize_and_normalize(' '.join(corpus['messages'].values.tolist()))\n",
    "        tokens = list(set(tokens))\n",
    "        classes = corpus['labels'].unique()\n",
    "        \n",
    "        frequency_table = pd.DataFrame(0,\n",
    "                                     index=classes,\n",
    "                                     columns=tokens)\n",
    "        \n",
    "        for clss in classes:\n",
    "            class_corpus = corpus[corpus['labels'] == clss]\n",
    "            all_class_tokens = tokenize_and_normalize(' '.join(class_corpus['messages'].values.tolist()))\n",
    "            for token in all_class_tokens:\n",
    "                frequency_table.at[clss, token] += 1\n",
    "#         print(frequency_table)\n",
    "        \n",
    "        return frequency_table, tokens\n",
    "    \n",
    "    def __calc_likelihoods_of_classes(self):\n",
    "        sum_of_frequencies_of_tokens_by_classes = self.frequency_table.sum(axis=1)\n",
    "        sum_of_frequencies_of_tokens_at_all = sum_of_frequencies_of_tokens_by_classes.sum(axis=0)\n",
    "        likelihoods_of_classes = sum_of_frequencies_of_tokens_by_classes / sum_of_frequencies_of_tokens_at_all\n",
    "        return likelihoods_of_classes\n",
    "    \n",
    "    def __calc_likelihoods_of_tokens(self):\n",
    "        sum_of_frequencies_of_tokens_by_tokens = self.frequency_table.sum(axis=0)\n",
    "        sum_of_frequencies_of_tokens_at_all = sum_of_frequencies_of_tokens_by_tokens.sum(axis=0)\n",
    "        likelihoods_of_tokens = sum_of_frequencies_of_tokens_by_tokens / sum_of_frequencies_of_tokens_at_all\n",
    "        return likelihoods_of_tokens\n",
    "    \n",
    "    def __calc_likelihoods_of_tokens_for_each_classes(self):\n",
    "        sum_of_frequencies_of_classes_by_classes = self.frequency_table.sum(axis=1)\n",
    "        likelihoods_of_tokens_for_each_classes = self.frequency_table.loc[:,:] \\\n",
    "                                                     .div(sum_of_frequencies_of_classes_by_classes, axis=0)\n",
    "        return likelihoods_of_tokens_for_each_classes\n",
    "    \n",
    "    def __calc_likelihoods_of_classes_for_each_tokens(self, tokens):\n",
    "\n",
    "#         p2 = 1;\n",
    "#         for token in list(set(tokens)):\n",
    "#             p2 *= self.likelihoods_of_tokens[token]\n",
    "        likelihoods_of_classes = pd.DataFrame(data=np.zeros((len(self.classes), 1), dtype=int),\n",
    "                     index=self.classes,\n",
    "                     columns=[\"likelihood\"])\n",
    "    \n",
    "        for clss in self.classes:\n",
    "            tokens_likelihoods = self.likelihoods_of_tokens_for_each_classes.loc[clss][tokens]\n",
    "            tokens_likelihoods_prod = tokens_likelihoods[tokens_likelihoods > 0].prod()\n",
    "        \n",
    "            likelihoods_of_classes.loc[clss] = self.likelihoods_of_classes[clss] * tokens_likelihoods_prod\n",
    "            \n",
    "        return likelihoods_of_classes\n",
    "    \n",
    "    def split_corpus(self, corpus):\n",
    "        train_corpus, test_corpus = split_dataset_data_frame(corpus)\n",
    "        return train_corpus, test_corpus\n",
    "    \n",
    "    def calc_accuracy(self, X, Y):\n",
    "        \"\"\"\n",
    "        Calculate the model accuracy. Predicted labels vs true ones.\n",
    "\n",
    "        Argument:\n",
    "        X -- a numpy array (labels) of shape (num_samples, 1 - a label). Usually, it's a matrix of predicted labels.\n",
    "        Y -- a numpy array (labels) of shape (num_samples, 1 - a label). Usually, it's a matrix of real labels.\n",
    "\n",
    "        Returns:\n",
    "        accuracy -- a classification accuracy\n",
    "        \"\"\"\n",
    "        accuracy = (np.copy(X) == np.copy(Y)).mean()\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare test corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinese Beijing Chinese</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chinese Chinese Shanghai</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinese Macao</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tokyo Japan Chinese</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   messages labels\n",
       "0   Chinese Beijing Chinese      0\n",
       "1  Chinese Chinese Shanghai      0\n",
       "2             Chinese Macao      0\n",
       "3       Tokyo Japan Chinese      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[\"Chinese Beijing Chinese\",\"0\"],\n",
    "            [\"Chinese Chinese Shanghai\",\"0\"], \n",
    "            [\"Chinese Macao\",\"0\"],\n",
    "            [\"Tokyo Japan Chinese\",\"1\"]])\n",
    "                \n",
    "data = pd.DataFrame(data=data[0:,0:],\n",
    "                  columns=[\"messages\",\"labels\"])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and validation a NaiveBayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create instance of NaiveBayes class\n",
    "nb = NaiveBayes()\n",
    "\n",
    "# Train our model\n",
    "# Tips: inside fit method it would be nice to split input data into train / test (80/20) sets and return modelâ€™ accuracy, e.g.:\n",
    "Accuracy = nb.fit(data)  # return accuracy \n",
    "\n",
    "# Try to predict class of text\n",
    "LIK = nb.predictOne(\"Chinese Chinese Chinese Tokyo Japan\")\n",
    "LIK\n",
    "# Must return[ ('Chinese Chinese Chinese Tokyo Japan', '0')]\n",
    "# pobability {'1': 0.00013548070246744226, '0': 0.00030121377997263036}\n",
    "# or log     {'1': -7.906681345001262, '0': -7.10769031284391}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb2 = NaiveBayes()\n",
    "train_corpus, test_corpus = nb.split_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb2.fit(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/anaconda3/lib/python3.6/site-packages/pandas/core/series.py:705: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    }
   ],
   "source": [
    "list_of_messages = test_corpus['messages'].values.tolist()\n",
    "predicted_corpus = nb2.predict(list_of_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4977578475336323"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = predicted_corpus.loc[:,'predicted classes'].values\n",
    "real = test_corpus.loc[:,'labels'].values\n",
    "nb2.calc_accuracy(predicted, real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
